{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning and variational inversion to quantify and attribute climate change (CIRC23)\n",
    "\n",
    "{opticon}`tag`\n",
    "{bdg-primary}`General`\n",
    "{bdg-secondary}`Modelling`\n",
    "{bdg-warning}`Special Issue`\n",
    "{bdg-info}`Python`\n",
    "\n",
    "<p align=\"left\">\n",
    "    <a href=\"https://github.com/eds-book-gallery/93463cac-471a-469d-ad52-0514fd9b67f2/blob/main/LICENSE\">\n",
    "        <img alt=\"license\" src=\"https://img.shields.io/badge/license-MIT-yellow.svg\">\n",
    "    </a>\n",
    "    <a href=\"https://notebooks.gesis.org/binder/v2/gh/eds-book-gallery/93463cac-471a-469d-ad52-0514fd9b67f2/postprint?labpath=notebook.ipynb\">\n",
    "        <img alt=\"binder\" src=\"https://mybinder.org/badge_logo.svg\">\n",
    "    </a>\n",
    "    <a href=\"https://github.com/eds-book-gallery/93463cac-471a-469d-ad52-0514fd9b67f2/actions/workflows/render.yaml\">\n",
    "        <img alt=\"render\" src=\"https://github.com/eds-book-gallery/93463cac-471a-469d-ad52-0514fd9b67f2/actions/workflows/render.yaml/badge.svg\">\n",
    "    </a>\n",
    "    <a href=\"https://github.com/alan-turing-institute/environmental-ds-book/issues/173\">\n",
    "        <img alt=\"review\" src=\"https://img.shields.io/badge/view-review-purple\">\n",
    "    </a>\n",
    "    <br/>\n",
    "</p>\n",
    "\n",
    "## Context\n",
    "### Purpose\n",
    "The purpose of the notebook is to explore the process and demonstrate the power of using a climate simulation and observation data with a convolutional neural network in the detection of climate change and a variational inversion approach to attribute forcings to changes in global mean surface temperatures from 1900 to 2014. \n",
    "\n",
    "### Description\n",
    "The modelling pipeline consists of the following three procedures: data acquisition & preprocessing, climate change detection and climate change attribution. \n",
    "* Observations (OBS) of the 2 m air temperature over the continents from HadCRUT4 (Morice et al., 2012) blended with sea surface temperature from HadISST412 (Rayner et al., 2003). The outputs of ocean-atmosphere general circulation models from the Coupled Model Intercomparison Project 6 (CMIP6) (Eyring et al., 2016) are also acquired and divided into several categories based on used forcings: single-forcing simulations utilizing greenhouse gas concentration (GHG), single-forcing simulations utilizing anthropogenic aerosols (AER), single-forcing simulations utilizing natural forcings such as volcanic aerosols and solar variations (NAT), and historical simulations using all the external forcings (HIST). The data is converted into an annual mean and is spatially averaged over the globe to obtain the global mean surface temperature (GMST). Then the mean temperatures of the pre-industrial time period (1850-1900) are removed from the period of interest (1900-2014) to obtain the temperature anomalies. \n",
    "* To determine the relation between the GMST of the single-forcing models GHG, AER, NAT to that of HIST, a convolutional neural network (CNN) is trained. The CNN uses the single-forcing model simulations as input of size (3,115), and uses the HIST simulations as the target of size (1,115). To estimate whether the added non-linearities of the CNN model improve the model performance, a simple linear model is trained using the same inputs and targets. \n",
    "* The trained CNN is then used to obtain the variational inversion using the backpropagation algorithm. In this case, OBS time series are used and linked to the single-forcing models (GHG, AER, NAT).         \n",
    "\n",
    "The model was implemented in Python 3.10 using PyTorch v1.13.0. Further details can be found in the Environmental Data Science paper [\"Detection and attribution of climate change: A deep learning and variational approach\"](https://doi.org/10.1017/eds.2022.17).\n",
    "\n",
    "### Highlights\n",
    "* Acquire, pre-process and visualize the observation and climate simulation data.\n",
    "* Train a CNN model to predict the historical simulation time series (HIST) given the three single-forcing model simulations (GHG, AER, NAT).\n",
    "* Use the trained CNN model combined with a variational approach to estimate the three forcings given observation (OBS) data.  \n",
    "\n",
    "### Abbreviations\n",
    "* OBS - observations\n",
    "* AER - single forcing simulations utilizing anthropogenic aerosols\n",
    "* GHG - single forcing simulations utilizing greenhouse gases\n",
    "* NAT - single forcing simulations utilizing natural forcings\n",
    "* HIST - historical simulations using all external forcings\n",
    "* CMIP6 - Coupled Model Intercomparison Project 6\n",
    "* GMST - Global Mean Surface Temperature\n",
    "* CNN - Convolutional Neural Network\n",
    "* MSE - Mean Squared Error\n",
    "\n",
    "### Contributions\n",
    "\n",
    "#### Notebook\n",
    "* Viktor Domazetoski (author), University of Göttingen, [@ViktorDomazetoski](https://github.com/ViktorDomazetoski) [ORCID](https://orcid.org/0000-0001-9830-7032)\n",
    "* Andrés Zúñiga-González (author), University of Cambridge, [@ancazugo](https://github.com/ancazugo)\n",
    "* Owen Allemang (author), University of Cambridge, [@SkirOwen](https://github.com/SkirOwen)\n",
    "\n",
    "#### Modelling codebase\n",
    "* Constantin Bône (author), UMR LOCEAN, Sorbonne Université, UMR ISIR, Sorbonne Université\n",
    "* Guillaume Gastineau (author), UMR LOCEAN, Sorbonne Université\n",
    "* Sylvie Thiria (author), UMR LOCEAN, Sorbonne Université\n",
    "* Patrick Gallinari (author), UMR ISIR, Sorbonne Université, Criteo AI Lab\n",
    "\n",
    "#### Modelling publications\n",
    "Bône, Constantin, Guillaume Gastineau, Sylvie Thiria, and Patrick Gallinari. \"Detection and attribution of climate change: A deep learning and variational approach.\" *Environmental Data Science* 1 (2022): e27.\n",
    "[https://doi.org/10.1017/eds.2022.17](https://doi.org/10.1017/eds.2022.17)\n",
    "\n",
    "### Data publications\n",
    "- Eyring, V., Bony, S., Meehl, G. A., Senior, C. A., Stevens, B., Stouffer, R. J., & Taylor, K. E. (2016). Overview of the Coupled Model Intercomparison Project Phase 6 (CMIP6) experimental design and organization. Geoscientific Model Development, 9(5), 1937-1958.\n",
    "- Morice, C. P., Kennedy, J. J., Rayner, N. A., & Jones, P. D. (2012). Quantifying uncertainties in global and regional temperature change using an ensemble of observational estimates: The HadCRUT4 data set. Journal of Geophysical Research: Atmospheres, 117(D8).\n",
    "- Rayner, N. A. A., Parker, D. E., Horton, E. B., Folland, C. K., Alexander, L. V., Rowell, D. P., ... & Kaplan, A. (2003). Global analyses of sea surface temperature, sea ice, and night marine air temperature since the late nineteenth century. Journal of Geophysical Research: Atmospheres, 108(D14).\n",
    "- Richardson, M., Cowtan, K., & Millar, R. J. (2018). Global temperature definition affects achievement of long-term climate goals. Environmental Research Letters, 13(5), 054004.\n",
    "\n",
    "### Source code\n",
    "The notebook contributors acknowledge the authors of the paper for providing a reproducible and public code available at https://gitlab.com/ConstantinBone/detection-and-attribution-of-climate-change-a-deep-learning-and-variational-approach. The source code of the paper was adapted to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "'''Math & Data Libraries'''\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import netCDF4 as nc4\n",
    "from scipy import signal\n",
    "import pooch\n",
    "\n",
    "''' ML Libraries'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "\n",
    "''' Miscellaneous Libraries'''\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "'''Visualization Libraries'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {\"grid.color\": \"0.5\", \"axes.edgecolor\": \"0.2\"})\n",
    "color_palette = [\"#FF8853\", \"#FFE174\", \"#007597\", \"#C1C36D\", \"#00A697\", \"#BC97E0\", \"#ffc0bf\"]\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of dictionaries containing the number of simulations per forcing per model\n",
    "model_dict = {\n",
    "\"IPSL\" : {'hist-GHG' : 10,'hist-aer' : 10, 'hist-nat' : 10,'historical' : 32, 'long_name': 'IPSL-CM6A-LR'},\n",
    "\"ACCESS\" : {'hist-GHG' : 3,'hist-aer' : 3, 'hist-nat' : 3,'historical' : 30, 'long_name': 'ACCESS-ESM1-5'},\n",
    "\"CESM2\" : {'hist-GHG' : 3,'hist-aer' : 2, 'hist-nat' : 3,'historical' : 11, 'long_name': 'CESM2'},\n",
    "\"BCC\" : {'hist-GHG' : 3,'hist-aer' : 3, 'hist-nat' : 3,'historical' : 3, 'long_name': 'BCC-CSM2-MR '},\n",
    "\"CanESM5\" : {'hist-GHG' : 50,'hist-aer' : 30, 'hist-nat' : 50,'historical' : 65, 'long_name': 'CanESM5'},\n",
    "\"FGOALS\" : {'hist-GHG' : 3,'hist-aer' : 3, 'hist-nat' : 3,'historical' : 6, 'long_name': 'FGOALs-g3'},\n",
    "\"HadGEM3\" : {'hist-GHG' : 4,'hist-aer' : 4, 'hist-nat' : 4,'historical' : 5, 'long_name': 'HadGEM3'},\n",
    "\"MIRO\" : {'hist-GHG' : 3,'hist-aer' : 3, 'hist-nat' : 3,'historical' : 50, 'long_name': 'MIROC6'},\n",
    "\"ESM2\" : {'hist-GHG' : 5,'hist-aer' : 5, 'hist-nat' : 5,'historical' : 7, 'long_name': 'MRI-ESM2'},\n",
    "\"NorESM2\" : {'hist-GHG' : 3,'hist-aer' : 3, 'hist-nat' : 3,'historical' : 3, 'long_name': 'NorESM2-LM'},\n",
    "\"CNRM\" : {'hist-GHG' : 9,'hist-aer' : 10, 'hist-nat' : 10,'historical' : 30, 'long_name': 'CNRM-CM6-1'},\n",
    "\"GISS\" : {'hist-GHG' : 10,'hist-aer' : 12, 'hist-nat' : 20,'historical' : 19, 'long_name': 'GISS-E2-1-G'}\n",
    "}\n",
    "\n",
    "model_list = ['CanESM5', 'CNRM', 'IPSL', 'ACCESS', 'BCC', 'FGOALS', 'HadGEM3', 'MIRO', 'ESM2','NorESM2','CESM2','GISS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n",
    "We use `torch.nn` and `torch.utils.data` modules to define linear (baseline) and CNN models, and datasets, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a Convolutional Neural Network. \n",
    "    The CNN consists of three convolutional layers, each with a padding of 5 \n",
    "    and a kernel size of 11 that are non-linearly transformed using a hyperbolic tangent function. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    size_channel : int\n",
    "        length of the layer (number of neurons) (defaults to 10)  \n",
    "    bias: boolean\n",
    "        whether to add a learnable bias to the input (defaults to True)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    forward(X):\n",
    "        Defines the computation performed at every call.\n",
    "    \"\"\"\n",
    "    def __init__(self, size_channel = 10, bias = True):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.conv1 = nn.Conv1d(3, size_channel, kernel_size=11, bias=bias, padding=5) # The input layer has a size of 3 due to the three forcings [ghg, aer, nat]\n",
    "        self.conv2 = nn.Conv1d(size_channel, size_channel, kernel_size=11, bias=bias, padding=5)\n",
    "        self.conv3 = nn.Conv1d(size_channel, 1, kernel_size=11, bias=bias, padding=5) # The output layer has a size of 1 as the target [hist]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "        The input goes through 2 convolutional and hyperbolic tangent layers before being \n",
    "        transformed to float and transformed to a tensor of shape (batch_size, N_years)\n",
    "        ---\n",
    "        Parameters\n",
    "        ----------\n",
    "            x : torch.tensor of shape (batch_size, N_forcings, N_years), where the batch size defaults to 100, N_forcings equals 3 and represents the three forcings [ghg, aer, nat], and N_years equals 115 and represents the data from 1900-2014.\n",
    "                input tensor containing the batch of data for the three forcings over the time span of interest.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            Output of computation : torch.tensor of shape (batch_size, N_years)\n",
    "                output tensor that tries to predict the historical simulations using all the external forcings as varying boundary conditions\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = x.float()\n",
    "        x = self.conv3(x)[:,0,:]\n",
    "        return x\n",
    "    \n",
    "class Linear_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a Simple Linear Module used for comparison. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    bias: boolean\n",
    "        whether to add a learnable bias to the input (defaults to False)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    forward(X):\n",
    "        Defines the computation performed at every call.\n",
    "    \"\"\"\n",
    "    def __init__(self, bias = False):\n",
    "        super(Linear_Model, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1, bias = bias)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "        The input goes through a linear layer before being transformed to a tensor of shape [batch_size, N_years]\n",
    "        ---\n",
    "        Parameters\n",
    "        ----------\n",
    "            x : torch.tensor of size (batch_size, N_forcings, N_years), where the batch size defaults to 100, N_forcings equals 3 and represents the three forcings [ghg, aer, nat], and N_years equals 115 and represents the data from 1900-2014.\n",
    "                input tensor containing the batch of data for the three forcings over the time span of interest.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            Output of computation : torch.tensor of shape (batch_size, N_years)\n",
    "                output tensor that tries to predict the historical simulations using all the external forcings as varying boundary conditions\n",
    "        \"\"\"\n",
    "        x = self.linear(X.transpose(1,2))\n",
    "        return x[:,:,0]\n",
    "\n",
    "class MonDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A pytorch Dataset class that is used as an input for the CNN DataLoader. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    ghg: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 3 to 50, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the greenhouse gas (GHG) single forcing simulations for each model over the time span of interest.\n",
    "    aer: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 3 to 50, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the aerosol (AER) single forcing simulations for each model over the time span of interest.\n",
    "    nat: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 2 to 30, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the natural (NAT) single forcing simulations for each model over the time span of interest.\n",
    "    historical: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 3 to 65, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the historical (HIST) simulations using all the external forcings as varying boundary conditions for each model over the time span of interest.\n",
    "    model_of_interest: int\n",
    "        The index of the model to exclude (include) when creating the train (test) dataset. Set to -1 in order to include all models.\n",
    "    dataset_type: str, one of [\"train\", \"test\"]\n",
    "        Whether to create a train or test dataset\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    len(N_samples):\n",
    "        Defines the number of samples returned.\n",
    "    __getitem__():\n",
    "        Fetch a random data sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, ghg, aer, nat, historical, model_of_interest=-1, dataset_type='train'):\n",
    "        self.ghg = ghg\n",
    "        self.aer = aer\n",
    "        self.nat = nat\n",
    "        self.historical = historical\n",
    "        self.dataset_type = dataset_type\n",
    "        self.model_of_interest = model_of_interest\n",
    "\n",
    "    def __len__(self, N_samples = 50000):\n",
    "        \"\"\"\n",
    "        Defines the number of samples returned.\n",
    "        ---\n",
    "        Parameters\n",
    "        ----------\n",
    "            N_samples: int\n",
    "                Number of samples to return. Defaults to 5e4.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "            Number of samples : int\n",
    "        \"\"\"\n",
    "        return N_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch a random data sample.\n",
    "        If self.model_of_interest is -1, no models are excluded and in both the train and test sample we randomly choose a model and then randomly choose a simulation for each forcing to add to the dataset.   \n",
    "        Otherwise, if we want to exclude a specific model, within the train set we randomly choose a model that is not the model we want to exclude, while in the test set we only acquire data from that model. \n",
    "        ---\n",
    "        Returns\n",
    "        -------\n",
    "            X : torch.tensor of shape (N_forcings, N_years), where N_forcings equals 3 and represents the three forcings [ghg, aer, nat], and N_years equals 115 and represents the data from 1900-2014.\n",
    "                a tensor containing a random sample of the simulation data for the three forcings over the time span of interest.\n",
    "            y : torch.tensor of shape (N_years, ) where N_years equals 115 and represents the data from 1900-2014\n",
    "                a tensor containing a random sample of the historical simulation data over the time span of interest.\n",
    "            model_idx : int\n",
    "                the index of the model used to acquire the random sample\n",
    "        \"\"\"\n",
    "        if(self.dataset_type == 'train'):\n",
    "            # We sample a random index between 0 and 11 until we find one that is not the model we want to exclude. \n",
    "            while True:\n",
    "                model_idx = random.randint(0, len(self.ghg) - 1)\n",
    "                if(model_idx != self.model_of_interest): \n",
    "                    break\n",
    "\n",
    "        elif(self.dataset_type == 'test'):\n",
    "            # We take the index of the model we want to include. \n",
    "            model_idx = self.model_of_interest\n",
    "            # If the model is -1, then all models can potentially be included, thus we randomly sample one of the 12 models\n",
    "            if(model_idx==-1):\n",
    "                model_idx = random.randint(0, len(self.ghg) - 1)\n",
    "\n",
    "        #We sample a simulation of each forcing of the model\n",
    "        ghg_sample = self.ghg[model_idx][random.randint(0, self.ghg[model_idx].shape[0] - 1)]\n",
    "        aer_sample = self.aer[model_idx][random.randint(0, self.aer[model_idx].shape[0] - 1)]\n",
    "        nat_sample = self.nat[model_idx][random.randint(0, self.nat[model_idx].shape[0] - 1)]\n",
    "        hist_sample = self.historical[model_idx][random.randint(0, self.historical[model_idx].shape[0] - 1)]\n",
    "        X = torch.stack((ghg_sample, aer_sample, nat_sample)).float()\n",
    "        y = hist_sample.float()\n",
    "\n",
    "        return X, y, model_idx\n",
    "\n",
    "class MonDataset_Inverse(Dataset):\n",
    "    \"\"\"\n",
    "    A pytorch Dataset class that is used as an input for the Inverse model DataLoader. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    ghg: torch.tensor of shape (N_simu, N_years) for one climate model. N_simu is dependent on the model and ranges from 3 to 50, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A tensor, containing the greenhouse gas (GHG) single forcing simulations for one model over the time span of interest.\n",
    "    aer: ltorch.tensor of shape (N_simu, N_years) for one climate model. N_simu is dependent on the model and ranges from 3 to 50, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A tensor, containing the aerosol (AER) single forcing simulations for for one model over the time span of interest.\n",
    "    nat: torch.tensor of shape (N_simu, N_years) for one climate model. N_simu is dependent on the model and ranges from 2 to 30, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A tensor, containing the natural (NAT) single forcing simulations for one model over the time span of interest.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    len(N_samples):\n",
    "        Defines the number of samples returned.\n",
    "    __getitem__():\n",
    "        Fetch a random data sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, ghg, aer, nat):\n",
    "        self.ghg = ghg\n",
    "        self.aer = aer\n",
    "        self.nat = nat\n",
    "\n",
    "    def __len__(self, N_samples = 100):\n",
    "        \"\"\"\n",
    "        Defines the number of samples returned.\n",
    "        ---\n",
    "        Parameters\n",
    "        ----------\n",
    "            N_samples: int\n",
    "                Number of samples to return. Defaults to 5e4.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "            Number of samples : int\n",
    "        \"\"\"\n",
    "        return N_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch a random data sample.\n",
    "        ---\n",
    "        Returns\n",
    "        -------\n",
    "            X : torch.tensor of shape (N_forcings, N_years), where N_forcings equals 3 and represents the three forcings [ghg, aer, nat], and N_years equals 115 and represents the data from 1900-2014.\n",
    "                a tensor containing a random sample of the simulation data for the three forcings over the time span of interest.\n",
    "        \"\"\"\n",
    "        \n",
    "        ghg_sample = self.ghg[random.randint(0, self.ghg.shape[0] - 1)]\n",
    "        aer_sample = self.aer[random.randint(0, self.aer.shape[0] - 1)]\n",
    "        nat_sample = self.nat[random.randint(0, self.nat.shape[0] - 1)]\n",
    "        X = torch.stack((ghg_sample, aer_sample, nat_sample)).float()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "We also define some utils functions to load models/simulations, train models and compute spatial calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(data):\n",
    "    \"\"\"\n",
    "    Calculate the spatially weighted mean over the globe. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.array of shape (N_years, latitude, longitude)\n",
    "        temporal data covering the entire globe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        obs_data_mean : np.array of shape (N_years)\n",
    "            Annual spatially averaged observation data from 1900 to 2020.\n",
    "    \"\"\"\n",
    "    data_mean = np.zeros((data.shape[0])) #Initialize np array to keep the results\n",
    "    latitude = np.linspace(-87.5, 87.5, 36) #Initialize array that contains the mean latitude coordinates of the 5 degree bands used in the spatial weighting\n",
    "    div = 0 \n",
    "\n",
    "    for j in range(36): #The latitude dimension of the data is split into 36 parts of 5 degrees which is why we loop throught 36 indices\n",
    "        for k in range(72): #Similarly, the longitude dimension of the data is split into 72 parts of 5 degrees which is why we loop throught 72 indices\n",
    "            data_mean += data[:,j,k] * np.cos(np.radians(latitude[j])) \n",
    "            div += np.cos(np.radians(latitude[j]))\n",
    "        data_mean /= div\n",
    "    return data_mean\n",
    "\n",
    "def get_observation_data(data_dir):\n",
    "    \"\"\"\n",
    "    Get annual observation (OBS) data of the 2 m air temperature over the continent from HadCRUT4 (Morice et al., 2012) \n",
    "    blended with sea surface temperature from HadISST4 (Rayner et al., 2003) over the time span of interest. The data is spatially averaged \n",
    "    ---\n",
    "    Returns\n",
    "    -------\n",
    "        obs_data_mean : np.array of shape (121, )\n",
    "            Annual spatially averaged observation data from 1900 to 2020.\n",
    "    \"\"\"\n",
    "    obs_nc = nc4.Dataset(data_dir + 'obs.nc', 'r')\n",
    "    obs_temperature_data = obs_nc.variables['temperature_anomaly'][:]\n",
    "    return calculate_spatial_mean(obs_temperature_data)\n",
    "\n",
    "def calculate_preindustrial_average(forcing, model_name = 'IPSL', physics = 1):\n",
    "    \"\"\"\n",
    "    Calculate pre-industrial average between years 1850-1900 for a given forcing and model.\n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    forcing : str: one of [\"hist-GHG\", \"hist-aer\", \"hist-nat\", \"historical\"]\n",
    "        type of forcing used in the simulation\n",
    "    model_name : str: one of the 12 models in the model_list\n",
    "        model used in the simulation\n",
    "    physics: int: one of [1, 2]\n",
    "        parameter used for the GISS model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        preindustrial_average : float\n",
    "            The pre-industrial average between the years 1850-1900.\n",
    "    \"\"\"\n",
    "\n",
    "    preindustrial_average = np.zeros((36,72))\n",
    "    # According to authors, for the GISS-E2-1-G, the temperature anomalies were computed separately for the \n",
    "    # simulations using two different physics, with different schemes to calculate the aerosols indirect impact\n",
    "    if(model_name==\"GISS\"): \n",
    "        if(type=='hist-aer'):\n",
    "            if(physics==1):\n",
    "                for i in range(model_dict[model_name][forcing]):\n",
    "                    if(i<5 or i>9):\n",
    "                        simu_nc = nc4.Dataset(f\"{data_dir}{model_name}_{forcing}_{i+1}.nc\", 'r')\n",
    "                        simu_temperature_data = simu_nc.variables['tas'][0:50]\n",
    "                        preindustrial_average += np.mean(simu_temperature_data, axis=0)/7\n",
    "            else:  \n",
    "                for i in range(model_dict[model_name][forcing]):\n",
    "                    if(i>=5 and i<=9):\n",
    "                        simu_nc = nc4.Dataset(f\"{data_dir}{model_name}_{forcing}_{i+1}.nc\", 'r')\n",
    "                        simu_temperature_data = simu_nc.variables['tas'][0:50]\n",
    "                        preindustrial_average += np.mean(simu_temperature_data, axis=0)/5\n",
    "        if(type=='historical'):\n",
    "            if(physics==1):\n",
    "                for i in range(model_dict[model_name][forcing]):\n",
    "                    if(i<10):\n",
    "                        simu_nc = nc4.Dataset(f\"{data_dir}{model_name}_{forcing}_{i+1}.nc\", 'r')\n",
    "                        simu_temperature_data = simu_nc.variables['tas'][0:50]\n",
    "                        preindustrial_average += np.mean(simu_temperature_data, axis=0)/10\n",
    "            else:\n",
    "                for i in range(model_dict[model_name][forcing]):\n",
    "                    if (i>=10):\n",
    "                        simu_nc = nc4.Dataset(f\"{data_dir}{model_name}_{forcing}_{i+1}.nc\", 'r')\n",
    "                        simu_temperature_data = simu_nc.variables['tas'][0:50]\n",
    "                        preindustrial_average += np.mean(simu_temperature_data, axis=0)/9\n",
    "        else:\n",
    "            for i in range(model_dict[model_name][forcing]):\n",
    "                if (i>=5 and i<=9):\n",
    "                    simu_nc = nc4.Dataset(f\"{data_dir}{model_name}_{forcing}_{i+1}.nc\", 'r')\n",
    "                    simu_temperature_data = simu_nc.variables['tas'][0:50]\n",
    "                    preindustrial_average += np.mean(simu_temperature_data, axis=0)/5\n",
    "    else:\n",
    "        for i in range(model_dict[model_name][forcing]):\n",
    "            simu_nc = nc4.Dataset(f\"{data_dir}{model_name}_{forcing}_{i+1}.nc\", 'r')\n",
    "            simu_temperature_data = simu_nc.variables['tas'][0:50]\n",
    "            preindustrial_average += np.mean(simu_temperature_data, axis=0)/model_dict[model_name][forcing]\n",
    "    return preindustrial_average\n",
    "\n",
    "def get_simulation(simulation_index, forcing, model_name = 'IPSL', filter = False):\n",
    "    \"\"\"\n",
    "    Get a simulation for a given forcing and model. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    simulation_index : int\n",
    "        the index of the requested simulation \n",
    "    forcing : str: one of [\"hist-GHG\", \"hist-aer\", \"hist-nat\", \"historical\"]\n",
    "        type of forcing used in the simulation\n",
    "    model_name : str: one of the 12 models in the model_list\n",
    "        model used in the simulation\n",
    "    filter : boolean. Defaults to False.\n",
    "        whether to apply a lowpass filter to the GHG and AER forcings \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        simulation : np.array of shape (N_years), where N_years equals 115\n",
    "            The data from one simulation for a given forcing and climate model for the years 1900-2014, from which the pre-industrial average is excluded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate pre-industrial average\n",
    "    physics = 1 \n",
    "    if(model_name=='GISS'):\n",
    "        if(forcing=='hist-aer' and (simulation_index>=6 and simulation_index<=10)):\n",
    "            physics = 2\n",
    "        elif(forcing=='historical' and simulation_index>10):\n",
    "            physics = 2\n",
    "    preindustrial_average = calculate_preindustrial_average(forcing, model_name = model_name, physics = physics)\n",
    "\n",
    "    # Calculate post-industrial 1900-2015 average\n",
    "    simu_nc = nc4.Dataset(f\"{data_dir}{model_name}_{forcing}_{simulation_index}.nc\", 'r')\n",
    "    simu_temperature_data = simu_nc.variables['tas'][50:]\n",
    "\n",
    "    # Subtract preindustrial average from the post-industrial data\n",
    "    simu_temperature_data = simu_temperature_data - preindustrial_average\n",
    "    simulation = calculate_spatial_mean(simu_temperature_data)\n",
    "    \n",
    "    if(filter):\n",
    "        if(forcing=='hist-GHG' or forcing=='hist-aer'):\n",
    "            b, a = signal.butter(20, 1/5, btype='lowpass') # Lowpass used in the filtrage of the data\n",
    "            simulation = signal.filtfilt(b, a, simulation)\n",
    "    return simulation\n",
    "\n",
    "def get_all_simulations(forcing, model_name = 'IPSL',filter = False):\n",
    "    \"\"\"\n",
    "    Get all simulations for a given forcing and model. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    forcing : str: one of [\"hist-GHG\", \"hist-aer\", \"hist-nat\", \"historical\"]\n",
    "        type of forcing used in the simulation\n",
    "    model_name : str: one of the 12 models in the model_list\n",
    "        model used in the simulation\n",
    "    filter : boolean. Defaults to False.\n",
    "        whether to apply a lowpass filter to the GHG and AER forcings \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        simulation_data : np.array of shape (N_simu, N_years), where N_simu is dependent on the model and ranges from 3 to 50, and N_years equals 115 and represents the data from 1900-2014 \n",
    "            The data from all simulations for a given forcing and climate model for the years 1900-2014, from which the pre-industrial average is excluded.\n",
    "    \"\"\"\n",
    "\n",
    "    simulation_data = np.zeros((model_dict[model_name][forcing], 115))\n",
    "    for i in range(model_dict[model_name][forcing]):\n",
    "        simulation_data[i] = get_simulation(i+1, forcing = forcing, model_name = model_name, filter = filter)[0:115]\n",
    "    return (simulation_data)\n",
    "\n",
    "def get_model_dataset(model_name = 'ALL', normalize = True, filter = False):\n",
    "    \"\"\"\n",
    "    Get the entire dataset (all simulations for all forcings) for a climate model or all climate models. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str: one of the 12 models in the model_list or \"ALL\" to acquire dataset for all climate models\n",
    "        model used in the simulation\n",
    "    normalize : boolean. Defaults to True.\n",
    "        whether to normalize all simulations by the maximum historical value \n",
    "    filter : boolean. Defaults to False.\n",
    "        whether to apply a lowpass filter to the GHG and AER forcings \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ghg: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 3 to 50, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the greenhouse gas (GHG) single forcing simulations for each model over the time span of interest.\n",
    "    aer: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 3 to 50, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the aerosol (AER) single forcing simulations for each model over the time span of interest.\n",
    "    nat: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 2 to 30, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the natural (NAT) single forcing simulations for each model over the time span of interest.\n",
    "    historical: list of length 12, with one torch.tensor of shape (N_simu, N_years) for each model. N_simu is dependent on the model and ranges from 3 to 65, and N_years equals 115 and represents the data from 1900-2014  \n",
    "        A list of tensors, containing the historical (HIST) simulations using all the external forcings as varying boundary conditions for each model over the time span of interest.\n",
    "    maximum_historical_list: list of length 1 or length 12, dependent if model_name==\"ALL\" \n",
    "        A list of maximum historical values per model    \n",
    "    \"\"\"\n",
    "    maximum_historical_list = []\n",
    "    \n",
    "    if (model_name == 'ALL'):\n",
    "        aer = []\n",
    "        ghg = []\n",
    "        nat = []\n",
    "        historical = []\n",
    "\n",
    "        for model_curr in tqdm(model_list):\n",
    "            print(model_curr)\n",
    "            aer_curr = torch.tensor(get_all_simulations('hist-aer', model_name = model_curr, filter = filter)[:, 0:115])\n",
    "            ghg_curr = torch.tensor(get_all_simulations('hist-GHG', model_name = model_curr, filter = filter)[:, 0:115])\n",
    "            nat_curr = torch.tensor(get_all_simulations('hist-nat', model_name = model_curr, filter = filter)[:, 0:115])\n",
    "            historical_curr = torch.tensor(get_all_simulations('historical', model_name = model_curr, filter = filter)[:, 0:115])\n",
    "            historical_max = torch.max(torch.mean(historical_curr, dim=0))\n",
    "            maximum_historical_list.append(historical_max)\n",
    "\n",
    "            if(normalize):\n",
    "                aer_curr = aer_curr / historical_max\n",
    "                ghg_curr = ghg_curr / historical_max\n",
    "                nat_curr = nat_curr / historical_max\n",
    "                historical_curr = historical_curr / historical_max\n",
    "\n",
    "            aer.append(aer_curr)\n",
    "            ghg.append(ghg_curr)\n",
    "            nat.append(nat_curr)\n",
    "            historical.append(historical_curr)\n",
    "\n",
    "    else:\n",
    "        aer = torch.tensor(get_all_simulations('hist-aer', model_name = model_name, filter = filter)[:,0:115])\n",
    "        ghg = torch.tensor(get_all_simulations('hist-GHG', model_name = model_name, filter = filter)[:,0:115])\n",
    "        nat = torch.tensor(get_all_simulations('hist-nat', model_name = model_name, filter = filter)[:,0:115])\n",
    "        historical = torch.tensor(get_all_simulations('historical', model_name = model_name, filter = filter)[:,0:115])\n",
    "        historical_max = torch.max(torch.mean(historical, dim=0))\n",
    "        maximum_historical_list.append(historical_max)\n",
    "\n",
    "        if(normalize):\n",
    "            aer = aer /historical_max\n",
    "            ghg = ghg / historical_max\n",
    "            nat = nat / historical_max\n",
    "            historical = historical/ historical_max\n",
    "\n",
    "    return ghg, aer, nat, historical, np.array(maximum_historical_list)\n",
    "    \n",
    "def train_models(train_dataloader, test_dataloader, lr = 0.001, N_epoch = 100, size_channel = 10, regularization = 0):\n",
    "    \"\"\"\n",
    "    Train the CNN and Linear model. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dataloader : pytorch Dataloader\n",
    "        the data loader for the training set that consists of a sample of simulations from the three forcings for random climate models.\n",
    "    test_dataloader : pytorch Dataloader\n",
    "        the data loader for the test set that consists of a sample of simulations from the three forcings for random climate models.\n",
    "    lr : int\n",
    "        learning rate. Defaults to 1e-3.\n",
    "    N_epoch : int\n",
    "        the number of epochs used to train the model. Defaults to 1e2.\n",
    "    size_channel : int\n",
    "        length of the layer (number of neurons) (defaults to 10)  \n",
    "    regularization : float\n",
    "        the amount of regularization used when training the model. Defaults to 0, representing no regularization. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_CNN : CNN_Model\n",
    "        a CNN model which takes data from the three single forcing models [ghg, aer, nat] and is trained to predict the historical (HIST) simulations using all the external forcings as varying boundary conditions\n",
    "    loss_train_CNN: list of length N_epoch\n",
    "        list that contains the MSE loss of the model on the training set after each epoch \n",
    "    loss_test_CNN: list of length N_epoch\n",
    "        list that contains the MSE loss of the model on the test set after each epoch \n",
    "    model_linear : Linear_Model\n",
    "        a benchmark linear model which takes data from the three single forcing models [ghg, aer, nat] and is trained to predict the historical (HIST) simulations using all the external forcings as varying boundary conditions\n",
    "    loss_train_linear: list of length N_epoch\n",
    "        list that contains the MSE loss of the model on the training set after each epoch \n",
    "    loss_test_linear: list of length N_epoch\n",
    "        list that contains the MSE loss of the model on the test set after each epoch \n",
    "    \"\"\"\n",
    "\n",
    "    model_CNN = CNN_Model(size_channel = size_channel, bias = True)\n",
    "    model_linear = Linear_Model(bias = False)\n",
    "\n",
    "    criterion_CNN = nn.MSELoss()\n",
    "    criterion_linear = nn.MSELoss()\n",
    "\n",
    "    optim_CNN = torch.optim.Adam(model_CNN.parameters(), lr=lr, weight_decay = regularization)\n",
    "    optim_linear = torch.optim.Adam(model_linear.parameters(), lr=lr, weight_decay=regularization)\n",
    "\n",
    "    loss_train_CNN = []\n",
    "    loss_test_CNN = []\n",
    "\n",
    "    loss_train_linear = []\n",
    "    loss_test_linear = []\n",
    "\n",
    "    for iter in tqdm(range(N_epoch)):\n",
    "        loss_total_train_CNN = 0\n",
    "        loss_total_test_CNN = 0\n",
    "        length_train_CNN = 0\n",
    "        length_test_CNN = 0\n",
    "\n",
    "        loss_total_train_linear = 0\n",
    "        loss_total_test_linear = 0\n",
    "        length_train_linear = 0\n",
    "        length_test_linear = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for(X_test, y_test, model_idx) in test_dataloader:\n",
    "                y_hat_test_CNN = model_CNN(X_test)\n",
    "                loss_test = criterion_CNN(y_hat_test_CNN.float(), y_test.float())\n",
    "                loss_total_test_CNN += loss_test\n",
    "                length_test_CNN += 1\n",
    "\n",
    "                y_hat_test_linear = model_linear(X_test)\n",
    "                loss_test = criterion_linear(y_hat_test_linear.float(),y_test.float())\n",
    "                loss_total_test_linear += loss_test\n",
    "                length_test_linear += 1\n",
    "\n",
    "        for(X_train, y_train, model_idx) in train_dataloader:\n",
    "            y_hat_train_CNN = model_CNN(X_train)\n",
    "            loss = criterion_CNN(y_hat_train_CNN.float(), y_train.float())\n",
    "            loss.backward()\n",
    "            optim_CNN.step()\n",
    "            loss_total_train_CNN += loss\n",
    "            optim_CNN.zero_grad()\n",
    "            length_train_CNN +=1\n",
    "\n",
    "            y_hat_train_linear = model_linear(X_train)\n",
    "            loss = criterion_linear(y_hat_train_linear.float(), y_train.float())\n",
    "            loss.backward()\n",
    "            optim_linear.step()\n",
    "            loss_total_train_linear += loss\n",
    "            optim_linear.zero_grad()\n",
    "            length_train_linear += 1\n",
    "\n",
    "        loss_total_train_CNN = loss_total_train_CNN.item() / length_train_CNN\n",
    "        loss_total_test_CNN = loss_total_test_CNN.item() / length_test_CNN\n",
    "        loss_total_train_linear = loss_total_train_linear.item() / length_train_linear\n",
    "        loss_total_test_linear = loss_total_test_linear.item() / length_test_linear\n",
    "\n",
    "        if(iter%10 == 0):\n",
    "            print(f\"Iteration {iter}:\")\n",
    "            print(f\"\\tCNN: training loss: {loss_total_train_CNN:.6f}, test loss {loss_total_test_CNN:.6f}\")\n",
    "            print(f\"\\tLinear: training loss: {loss_total_train_linear:.6f}, test loss {loss_total_test_linear:.6f}\")\n",
    "\n",
    "        loss_train_CNN.append(loss_total_train_CNN)\n",
    "        loss_test_CNN.append(loss_total_test_CNN)\n",
    "\n",
    "        loss_train_linear.append(loss_total_train_linear)\n",
    "        loss_test_linear.append(loss_total_test_linear)\n",
    "\n",
    "    return model_CNN, np.array(loss_train_CNN), np.array(loss_test_CNN), model_linear, np.array(loss_train_linear), np.array(loss_test_linear)\n",
    "\n",
    "def train_inverse_model(inputs, target, model, max_iter = 100000, alpha = 0.005):\n",
    "    \"\"\"\n",
    "    Perform backpropagation of the CNN model to obtain the variational inverse model estimates. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : torch.tensor() of shape (3,115)\n",
    "        a tensor that consists of a sample of simulations from the three forcings for random climate models.\n",
    "    target : torch.tensor() of shape (1,115)\n",
    "        a tensor that consists of the observation data\n",
    "    model : CNN_Model()\n",
    "        the convolutional model trained to predict HIST given [ghg, aer, nat] as input\n",
    "    max_iter : int\n",
    "        the maximum number of iterations used in the model. Defaults to 1e5.\n",
    "    alpha : float\n",
    "        the loss threshold to perform early model stopping. Defaults to 0.005.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : torch.tensor()\n",
    "        The input that corresponds to the OBS data as determined by the inverse model \n",
    "    output_estimate: torch.tensor()\n",
    "        The model prediction for the determined input \n",
    "    \"\"\"\n",
    "    X = Variable(inputs.clone().detach(), requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([X], lr=0.0001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        output_estimate = model(X) \n",
    "        MSE_loss = criterion(output_estimate.float(), target.float())\n",
    "        penalization_term_loss = criterion(X.float(), inputs.float())\n",
    "        loss = MSE_loss + 0.01 * penalization_term_loss\n",
    "\n",
    "        if(iter%10000 == 0):\n",
    "            print(f\"Iteration {iter}:\")\n",
    "            print(f\"\\tInverse model: MSE(OBS, CNN(X)): {MSE_loss:.6f}, MSE(X, inputs) {penalization_term_loss:.6f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if(MSE_loss<alpha):\n",
    "            print(f\"Model achieved loss threshold at iteration {iter}:\")\n",
    "            break\n",
    "\n",
    "    return X, output_estimate\n",
    "\n",
    "def get_inverse_model_estimates(observation_data, model, normalize = True, denormalize = True, filter = False):\n",
    "    \"\"\"\n",
    "    Train the CNN and Linear model. \n",
    "    ---\n",
    "    Parameters\n",
    "    ----------\n",
    "    observation_data : torch.tensor\n",
    "        a tensor that consists of the observation data\n",
    "    model : CNN_Model\n",
    "        the convolutional model trained to predict HIST given [ghg, aer, nat] as input\n",
    "    normalize : boolean. Defaults to True.\n",
    "        whether to normalize all simulations by the maximum historical value \n",
    "    denormalize : boolean. Defaults to True.\n",
    "        whether to denormalize all simulations by multiplying by the maximum historical value \n",
    "    filter : boolean. Defaults to False.\n",
    "        whether to apply a lowpass filter to the GHG and AER forcings \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_list_input: np.array of shape (N_models, N_forcings, N_years), where N_models = 12, N_forcings = 3, N_years = 115\n",
    "        array that contains the inverse model input estimates of the [ghg, aer, nat] by backpropagating using the observation data\n",
    "    results_list_output: np.array of shape (N_models, 1, N_years), where N_models = 12, N_years = 115\n",
    "        array that contains the inverse model output estimates\n",
    "    \"\"\"\n",
    "    \n",
    "    results_list_input = []\n",
    "    results_list_output = []\n",
    "\n",
    "    for i in tqdm(range(len(model_list))):\n",
    "        ghg, aer, nat, historical, maximum_historical_list = get_model_dataset(model_name = model_list[i], normalize = True, filter = filter)\n",
    "\n",
    "        dataloader_inverse_model = DataLoader(MonDataset_Inverse(ghg, aer, nat), batch_size = 1)\n",
    "\n",
    "        for inputs in dataloader_inverse_model:\n",
    "            X, output_estimate = train_inverse_model(inputs, observation_data, model, max_iter = 100000)\n",
    "\n",
    "            X = X.clone().detach().numpy()\n",
    "            output_estimate = output_estimate.clone().detach().numpy()\n",
    "\n",
    "            if denormalize:\n",
    "                X *= maximum_historical_list[-1]\n",
    "                output_estimate *= maximum_historical_list[-1]\n",
    "\n",
    "            results_list_input.append(X)\n",
    "            results_list_output.append(output_estimate)\n",
    "\n",
    "    results_list_input = np.array(results_list_input)[:, 0]\n",
    "    results_list_output = np.array(results_list_output)[:, 0]\n",
    "\n",
    "    with open('./Results/Inverse_Results.npy', 'wb') as f1:\n",
    "        np.save(f1, results_list_input)\n",
    "        \n",
    "    return results_list_input, results_list_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be found in a compressed file data_pre_ind_2 at https://gitlab.com/ConstantinBone/detection-and-attribution-of-climate-change-a-deep-learning-and-variational-approach. We use `pooch` to fetch and unzip them directly from the GitLab repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "pooch.retrieve(\n",
    "    url=\"https://gitlab.com/ConstantinBone/detection-and-attribution-of-climate-change-a-deep-learning-and-variational-approach/-/raw/main/data_pre_ind_2.zip\",\n",
    "    known_hash=None,\n",
    "    processor=pooch.Unzip(extract_dir='data'),\n",
    "    path=f\".\",\n",
    "    progressbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by acquiring the annual OBS data of the 2 m air temperature over the continents from HadCRUT4 blended with sea surface temperature from HadISST4. The data is spatially averaged over the globe and only the time span of interest (1900 - 2014) is acquired. To make OBS and HIST comparable, OBS is corrected of its blending effects using a 1.06 multiplier coefficient (Richardson et al., 2018). Finally, the OBS data is normalized to a maximum of 1 by dividing the series with the OBS maximum.\n",
    "\n",
    "We then get the simulation data for all forcings (GHG, AER, NAT, HIST) of all 12 climate models. The simulation data is spatially averaged over the globe to create an annual mean from which the pre-industrial average is subtracted. The simulations are then also max-normalized by dividing with the historical maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input data dir\n",
    "data_dir = 'data/data_pre_ind_2/'\n",
    "\n",
    "# Get observation data\n",
    "observation_data = torch.tensor(get_observation_data(data_dir))[0:115] * 1.06\n",
    "observation_max = torch.max(observation_data)\n",
    "observation_data = observation_data /observation_max\n",
    "\n",
    "# Get the simulation data for all forcings and all climate models\n",
    "ghg, aer, nat, historical, maximum_historical_list = get_model_dataset(model_name = 'ALL', normalize = True, filter = False)\n",
    "\n",
    "N_simu_table_list = []\n",
    "for i in range(len(model_list)):\n",
    "    N_simu_table_list.append([model_dict[model_list[i]]['long_name'], ghg[i].shape[0], aer[i].shape[0], nat[i].shape[0], historical[i].shape[0]])\n",
    "N_simu_table = pd.DataFrame(N_simu_table_list, columns = [\"Model\", \"GHG\", \"AER\", \"NAT\", \"HIST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "table1_models_fig = plt.figure(figsize = (8, .16))\n",
    "\n",
    "rcolors = plt.cm.BuPu(np.full(len(N_simu_table.index), 0.3))\n",
    "ccolors = plt.cm.BuPu(np.full(len(N_simu_table.columns), 0.3))\n",
    "\n",
    "plt.table(cellText = N_simu_table.values, rowLabels = N_simu_table.index, \n",
    "          colLabels = N_simu_table.columns, cellLoc='center',                      \n",
    "          rowColours = rcolors, colColours = ccolors)\n",
    "\n",
    "#add title to table\n",
    "plt.title('Number of Simulations per Model per Forcing')\n",
    "#turn axes off\n",
    "plt.axis('off')\n",
    "table1_models_fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the intra-specific and inter-specific variation of the simulations for the 12 climate models, we plot the three single-forcing simulations: GHG (orange), AER (yellow), NAT (blue) and the historical simulation using all external forcings: HIST (olive) for each model. The mean value of the simulations per forcing are shown with a solid line, while the shaded color represents one standard deviation off the mean. The observation data: OBS is also shown with a black dotted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig1_models, ax = plt.subplots(figsize = (16,10), ncols = 4, nrows = 3)\n",
    "plt.suptitle(\"Visualization of the simulations of the 12 models\", fontsize = 20)\n",
    "for i in range(len(model_list)):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.title(model_dict[model_list[i]]['long_name'])\n",
    "    for j, (forcing, forcing_name) in enumerate(zip([ghg, aer, nat, historical], [\"GHG\", \"AER\", \"NAT\", \"HIST\"])):\n",
    "        forcing_mean = torch.mean(forcing[i], axis=0)\n",
    "        forcing_std = torch.std(forcing[i], axis=0)\n",
    "\n",
    "        plt.plot(forcing_mean, color = color_palette[j], linewidth = 3, alpha = 0.8, label = forcing_name)\n",
    "        plt.fill_between(np.arange(115), forcing_mean - forcing_std, forcing_mean + forcing_std, color = color_palette[j], alpha = 0.3)\n",
    "    plt.plot(observation_data, color = \"#444444\", linestyle = \":\", linewidth = 3, alpha = 1, label = \"OBS\")\n",
    "\n",
    "    plt.ylim(-2.5, 2.5)\n",
    "    plt.xticks(np.linspace(0, 120, 5), [str(tick+1900) for tick in np.linspace(0, 120, 5, dtype=int)])\n",
    "    plt.yticks(np.linspace(-2.5, 2.5, 5), [str(np.round(tick, 2)) for tick in np.linspace(-2.5, 2.5, 5, dtype=float)])\n",
    "\n",
    "    if(i==0):\n",
    "        handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "        plt.legend(handles, [\"GHG\", \"AER\", \"NAT\", \"HIST\", \"OBS\"], loc = \"upper left\", fontsize = 10)\n",
    "    if(i%4==0):\n",
    "        plt.ylabel(f\"GMST (\\N{DEGREE SIGN}C)\", fontsize = 13)\n",
    "    if(i>7):\n",
    "        plt.xlabel(f\"Years\", fontsize = 13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the variation of the forcings across all 12 climate models. The same color code as the previous figure is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig_models_mean, ax = plt.subplots(figsize = (8,5), ncols = 1, nrows = 1)\n",
    "plt.suptitle(\"Mean and standard deviation of the simulations of the 12 models\", fontsize = 20)\n",
    "for j, (forcing, forcing_name) in enumerate(zip([ghg, aer, nat, historical], [\"GHG\", \"AER\", \"NAT\", \"HIST\"])):\n",
    "    forcing_mean = torch.mean(torch.cat(forcing), axis=0)\n",
    "    forcing_std = torch.std(torch.cat(forcing), axis=0)\n",
    "        \n",
    "    plt.plot(forcing_mean, color = color_palette[j], linewidth = 3, alpha = 0.8, label = forcing_name)\n",
    "    plt.fill_between(np.arange(115), forcing_mean - forcing_std, forcing_mean + forcing_std, color = color_palette[j], alpha = 0.3)\n",
    "\n",
    "plt.plot(observation_data, color = \"#444444\", linestyle = \":\", linewidth = 3, alpha = 1, label = \"OBS\")\n",
    "\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.xticks(np.linspace(0, 120, 5), [str(tick+1900) for tick in np.linspace(0, 120, 5, dtype=int)])\n",
    "plt.yticks(np.linspace(-2.5, 2.5, 5), [str(np.round(tick, 2)) for tick in np.linspace(-2.5, 2.5, 5, dtype=float)])\n",
    "\n",
    "plt.ylabel(f\"GMST (\\N{DEGREE SIGN}C)\", fontsize = 13)\n",
    "plt.xlabel(f\"Years\", fontsize = 13)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, [\"GHG\", \"AER\", \"NAT\", \"HIST\", \"OBS\"], loc = \"upper left\", fontsize = 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### Model training\n",
    "To train the model for the detection of climate change given the single-forcing simulations, we first define a custom dataloader. The data loader works such that it randomly picks a model and a simulation index to create a data sample of input (GHG, AER, NAT) and a target (HIST).\n",
    "\n",
    "```python\n",
    "BATCH_SIZE = 100\n",
    "train_dataloader = DataLoader(MonDataset(ghg, aer, nat, historical, model_of_interest = -1, dataset_type = 'train'), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(MonDataset(ghg, aer, nat, historical, model_of_interest = -1, dataset_type = 'test'), shuffle=True, batch_size=BATCH_SIZE)\n",
    "```\n",
    "\n",
    "The train and test dataloaders are then used as input to a convolutional network with three convolutional layers and a hyperbolic tangent activation function. As a result of the function, we receive the trained CNN model, as well as a linear model which is used for comparison. \n",
    "\n",
    "```python\n",
    "model_CNN, loss_train_CNN, loss_test_CNN, model_linear, loss_train_linear, loss_test_linear = train_models(train_dataloader, test_dataloader, N_epoch = 100)\n",
    "\n",
    "## Save training/test curves\n",
    "loss_df = pd.DataFrame({'loss_train_CNN': loss_train_CNN, 'loss_train_linear': loss_train_linear,\n",
    "                 'loss_test_CNN': loss_test_CNN, 'loss_test_linear': loss_test_linear})\n",
    "loss_df.to_csv('outputs/results/models_loss_history.csv')\n",
    "\n",
    "## Save models\n",
    "pickle.dump(model_CNN, open('Models/CNN_model.pkl', 'wb'))\n",
    "pickle.dump(model_linear, open('Models/Linear_model.pkl', 'wb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load models\n",
    "model_linear = pickle.load(open('outputs/models/Linear_model.pkl', 'rb'))\n",
    "model_CNN = pickle.load(open('outputs/models/CNN_model.pkl', 'rb'))\n",
    "\n",
    "## Load loss curves\n",
    "loss_curves_df = pd.read_csv('outputs/models/models_loss_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "skip_first_epochs = 5\n",
    "\n",
    "fig_train_history, ax = plt.subplots(figsize = (8,5), ncols = 1, nrows = 1)\n",
    "plt.suptitle(\"Loss per Epoch of the CNN and Linear models\", fontsize = 20)\n",
    "\n",
    "for i, (loss_list, dataset_type) in enumerate(zip([loss_curves_df['loss_train_CNN'], loss_curves_df['loss_test_CNN']], [\"Train\", \"Test\"])):\n",
    "    plt.plot(loss_list[skip_first_epochs:], color = color_palette[i+1], linestyle = \"-\", linewidth = 3, alpha = 0.8, label = f\"CNN Model {dataset_type} Loss\")\n",
    "\n",
    "for i, (loss_list, dataset_type) in enumerate(zip([loss_curves_df['loss_train_linear'], loss_curves_df['loss_test_linear']], [\"Train\", \"Test\"])):\n",
    "    plt.plot(loss_list[skip_first_epochs:], color = color_palette[i+1], linestyle = \"--\", linewidth = 3, alpha = 0.8, label = f\"Linear Model {dataset_type} Loss\")\n",
    "\n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc = \"upper right\", fontsize = 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use trained CNN model to estimate the contribution of each forcing in the multi-model dataset using variational inversion. This is done by using the OBS data as the target variable of the CNN model and backpropagating until we get estimates of the input.\n",
    "\n",
    "```python\n",
    "results_list_input, results_list_output = get_inverse_model_estimates(observation_data, model_CNN, normalize = True, denormalize = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inverse model results\n",
    "results_list_input = np.load('./outputs/results/Inverse_Results.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the variational inverse model estimates for each model compared to the climate model simulations. We plot the three single-forcing simulations: GHG (orange), AER (yellow), NAT (blue) for the inverse estimates (dashed line) and the original climate model simulations (solid line). The color shades show one standard deviation across the inversion (dashed area) and the simulations. The observation data: OBS is also shown with a black dotted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig_inverse_models, ax = plt.subplots(figsize = (16,10), ncols = 4, nrows = 3)\n",
    "plt.suptitle(\"Mean and standard deviation of the inverse model estimates of the simulations\", fontsize = 20)\n",
    "for i in range(len(model_list)):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.title(model_dict[model_list[i]]['long_name'])\n",
    "    for j, (forcing, forcing_name) in enumerate(zip([ghg, aer, nat], [\"GHG\", \"AER\", \"NAT\"])):\n",
    "        forcing_mean = torch.mean(forcing[j], axis=0)\n",
    "        forcing_std = torch.std(forcing[j], axis=0)\n",
    "\n",
    "        forcing_mean_inverse = np.mean(results_list_input[100*i:100*(i+1)], axis=0)[j]\n",
    "        forcing_std_inverse = np.std(results_list_input[100*i:100*(i+1)], axis=0)[j]\n",
    "        \n",
    "        plt.plot(forcing_mean, color = color_palette[j], linestyle = \"-\", linewidth = 3, alpha = 0.8, label = forcing_name)\n",
    "        plt.fill_between(np.arange(115), forcing_mean - forcing_std, forcing_mean + forcing_std, color = color_palette[j], alpha = 0.3)\n",
    "\n",
    "        plt.plot(forcing_mean_inverse, color = color_palette[j], linestyle = \"--\", linewidth = 3, alpha = 0.8, label = forcing_name)\n",
    "        plt.fill_between(np.arange(115), forcing_mean_inverse - forcing_std_inverse, forcing_mean_inverse + forcing_std_inverse, hatch='////', color = \"black\", facecolor = color_palette[j], alpha = 0.3)\n",
    "        \n",
    "    plt.plot(observation_data, color = \"#444444\", linestyle = \":\", linewidth = 3, alpha = 1, label = \"OBS\")\n",
    "\n",
    "    plt.ylim(-2.5, 2.5)\n",
    "    plt.xticks(np.linspace(0, 120, 5), [str(tick+1900) for tick in np.linspace(0, 120, 5, dtype=int)])\n",
    "    plt.yticks(np.linspace(-2.5, 2.5, 5), [str(np.round(tick, 2)) for tick in np.linspace(-2.5, 2.5, 5, dtype=float)])\n",
    "\n",
    "    if(i==0):\n",
    "        handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "        plt.legend(handles, [\"GHG\", \"AER\", \"NAT\", \"HIST\", \"OBS\"], loc = \"upper left\", fontsize = 10)\n",
    "    if(i%4==0):\n",
    "        plt.ylabel(f\"GMST (\\N{DEGREE SIGN}C)\", fontsize = 13)\n",
    "    if(i>7):\n",
    "        plt.xlabel(f\"Years\", fontsize = 13)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure visualizes the same figure by taking all models into account. The same color as the previous figure is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig_inverse_models_mean, ax = plt.subplots(figsize = (8,5), ncols = 1, nrows = 1)\n",
    "plt.suptitle(\"Mean and standard deviation of the inverse model estimates of the simulations\", fontsize = 20)\n",
    "for i, (forcing, forcing_name)  in enumerate(zip([ghg, aer, nat], [\"GHG\", \"AER\", \"NAT\"])):\n",
    "    forcing_std = torch.std(torch.cat(forcing), axis=0)\n",
    "    forcing_mean = torch.mean(torch.cat(forcing), axis=0)\n",
    "\n",
    "    forcing_mean_inverse = np.mean(results_list_input, axis=0)[i]\n",
    "    forcing_std_inverse = np.std(results_list_input, axis=0)[i]\n",
    "\n",
    "    plt.plot(forcing_mean, color = color_palette[i], linestyle = \"-\", linewidth = 3, alpha = 0.8, label = forcing_name)\n",
    "    plt.fill_between(np.arange(115), forcing_mean - forcing_std, forcing_mean + forcing_std, color = color_palette[i], alpha = 0.3)\n",
    "\n",
    "    plt.plot(forcing_mean_inverse, color = color_palette[i], linestyle = \"--\", linewidth = 3, alpha = 0.8, label = forcing_name)\n",
    "    plt.fill_between(np.arange(115), forcing_mean_inverse - forcing_std_inverse, forcing_mean_inverse + forcing_std_inverse, hatch='////', color = \"black\", facecolor = color_palette[i], alpha = 0.3)\n",
    "\n",
    "plt.plot(observation_data, color = \"#444444\", linestyle = \":\", linewidth = 3, alpha = 1, label = \"OBS\")\n",
    "\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.xticks(np.linspace(0, 120, 5), [str(tick+1900) for tick in np.linspace(0, 120, 5, dtype=int)])\n",
    "plt.yticks(np.linspace(-2.5, 2.5, 5), [str(np.round(tick, 2)) for tick in np.linspace(-2.5, 2.5, 5, dtype=float)])\n",
    "plt.xlabel(f\"Years\", fontsize = 13)\n",
    "plt.ylabel(f\"GMST (\\N{DEGREE SIGN}C)\", fontsize = 13)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, [\"GHG\", \"Inversion of GHG\", \"AER\", \"Inversion of AER\", \"NAT\", \"Inversion of NAT\", \"OBS\"], loc = \"upper left\", fontsize = 10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the mean and standard deviation of the variational inverse model estimates over time, and further show the results for two years, in this case 1993 and 2014 for greenhouse gases (orange), anthropogenic aerosols (yellow) and natural forcings (blue). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig2_inverse_estimates, ax = plt.subplots(figsize = (12.4, 4.8), ncols = 3, nrows = 1, gridspec_kw = {'width_ratios' : [3,1,1]})\n",
    "plt.suptitle(\"Mean and standard deviation of the inverse model estimates of the simulations\", fontsize = 20)\n",
    "plt.subplot(1,3,1)\n",
    "for i, (forcing, forcing_name)  in enumerate(zip([ghg, aer, nat], [\"GHG\", \"AER\", \"NAT\"])):\n",
    "    forcing_mean_inverse = np.mean(results_list_input, axis=0)[i]\n",
    "    forcing_std_inverse = np.std(results_list_input, axis=0)[i]\n",
    "\n",
    "    plt.plot(forcing_mean_inverse, color = color_palette[i], linestyle = \"--\", linewidth = 3, alpha = 0.8, label = forcing_name)\n",
    "    plt.fill_between(np.arange(115), forcing_mean_inverse - forcing_std_inverse, forcing_mean_inverse + forcing_std_inverse, color = color_palette[i], alpha = 0.3)\n",
    "\n",
    "plt.plot(observation_data, color = \"#444444\", linestyle = \":\", linewidth = 3, alpha = 1, label = \"OBS\")\n",
    "\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.xticks(np.linspace(0, 120, 5), [str(tick+1900) for tick in np.linspace(0, 120, 5, dtype=int)])\n",
    "plt.yticks(np.linspace(-2.5, 2.5, 5), [str(np.round(tick, 2)) for tick in np.linspace(-2.5, 2.5, 5, dtype=float)])\n",
    "plt.xlabel(f\"Years\", fontsize = 13)\n",
    "plt.ylabel(f\"GMST (\\N{DEGREE SIGN}C)\", fontsize = 13)\n",
    "\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "plt.legend(handles, [\"Inversion of GHG\", \"Inversion of AER\", \"Inversion of NAT\", \"OBS\"], loc = \"upper left\", fontsize = 10)\n",
    "\n",
    "for j, year in enumerate([1993, 2014]):\n",
    "    plt.subplot(1,3,j+2)\n",
    "    year_ind = year - 1900\n",
    "    for i, forcing_name  in enumerate([\"GHG\", \"AER\", \"NAT\"]):\n",
    "        result_forcing_year = results_list_input[:, i, year_ind] \n",
    "        # bin = int(round((np.max(result_forcing_year) - np.min(result_forcing_year)) * 10)) +1\n",
    "        plt.hist(results_list_input[:, i, year_ind], bins = np.linspace(-2.5, 2.5, 30), alpha = 0.5, color = color_palette[i], orientation = \"horizontal\")\n",
    "        \n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.title(year)\n",
    "        plt.xlabel(f\"Frequency\", fontsize = 13)\n",
    "        plt.ylabel(f\"GMST (\\N{DEGREE SIGN}C)\", fontsize = 13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the left panel of the image, the results are comparable to the original simulations, however, with a slightly increased standard deviation. The right panel illustrates the distribution of the results from the inversions in 1993 and 2014, showing a clear delineation between the effect of GHG compared to AER and NAT forcings. Furthermore, this results obtained in this run of the notebook slighly differ from the ones in the original paper. This could be due to multiple reasons, such as the stochasticity involved in training the models or slight differences in the pipeline compared to the original paper.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "- Data uncertainty: The analysis relies on the the accuracy and quality of the data which is used to train the models. However, especially when using the climate simulation data (GHG, AER, NAT, HIST), we rely on the use of climate models which inherently carry uncertainties due to simplified representations of complex climate processes, thus can bias the changes to specific forcings. \n",
    "- Spatiotemporal resolution: The analysis provides GMST attribution on a global scale, but it may not capture regional variations adequately. Regional climate responses and localized forcings might not be fully accounted for, limiting the ability to attribute changes at finer spatial scales. However, this is something the authors of the paper discuss they will research in future work.\n",
    "- Incomplete forcings: The analysis focuses on attributing GMST changes to well-mixed greenhouse gases, anthropogenic aerosols, and natural forcings. However, other external factors, such as land-use changes or solar variability, may also contribute to GMST variations and are not explicitly considered in this analysis.\n",
    "- Future projection uncertainty: The analysis focuses on historical data up to 2014, and the attribution results may not fully reflect potential changes beyond that period. Projecting the attribution results into the future introduces additional uncertainties related to emission scenarios and climate model projections.\n",
    "- Sensitivity to methodology: The results of the analysis may be sensitive to the specific methodology chosen, including the design of the variational inversion approach and the choice of climate models used. Alternative methodologies could yield different attribution outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- The climate change attribution problem is a very important issue that pertains to understanding the exact causes behind climate change. However, most approaches tackling this problem have relied on the classical forcing additivity assumption which precludes any non-linear combinations that are ubiquitous in nature.\n",
    "- In this notebook, we utilize a CNN architecture to escape this constraint and estimate the historical global mean surface temperature using single forcing simulations (greenhouse gases, anthropogenic aerosols and natural forcings) from ocean-atmosphere general circulation models from the Coupled Model Intercomparison Project 6 (CMIP6). \n",
    "- The results show that the CNN performs better and obtains a lower mean square error compared to a simpler linear model trained on the same data. \n",
    "- The trained CNN is then used with a variational inversion approach to estimate the single forcings when given observation data from HadCRUT4 and HAdISST412. The estimated inversions of the single forcings show high similarity to the original data and provide coherent results with larger confidence intervals.          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "**Dataset**: Bône, Constantin. (2023).  Detection and attribution of climate change a Deep Learning and Variational approach. https://gitlab.com/ConstantinBone/detection-and-attribution-of-climate-change-a-deep-learning-and-variational-approach/-/blob/main/data_pre_ind_2.zip - Commit 8dff0335be7dbf07e56c7a99bef6b12453360012\n",
    "\n",
    "**License**: The code in this notebook is licensed under the MIT License. The Environmental Data Science book is licensed under the Creative Commons by Attribution 4.0 license. See further details [here](https://github.com/alan-turing-institute/environmental-ds-book/blob/master/LICENSE.md).\n",
    "\n",
    "**Contact**: If you have any suggestion or report an issue with this notebook, feel free to [create an issue](https://github.com/alan-turing-institute/environmental-ds-book/issues/new/choose) or or send a direct message to environmental.ds.book@gmail.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "print(f'Last tested: {date.today()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Outputs registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "outputs = {\n",
    "    'static_figures': {\n",
    "        'filenames': ['table1_models', 'fig1_models', 'fig_models_mean','fig_train_history', 'fig_inverse_models', 'fig_inverse_models_mean', 'fig2_inverse_estimates'], \n",
    "        'data':[table1_models_fig, fig1_models, fig_models_mean, fig_train_history, fig_inverse_models, fig_inverse_models_mean, fig2_inverse_estimates]}\n",
    "}\n",
    "\n",
    "fig_folder = 'outputs/figures'\n",
    "os.makedirs(fig_folder, exist_ok=True)\n",
    "    \n",
    "#save static figures\n",
    "if len(outputs['static_figures']['filenames']) > 0:\n",
    "    [data.savefig(os.path.join(fig_folder,outputs['static_figures']['filenames'][x]  + '.png'), bbox_inches=\"tight\") for x, data in enumerate(outputs['static_figures']['data'])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
